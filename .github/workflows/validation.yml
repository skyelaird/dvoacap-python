name: DVOACAP Validation

on:
  push:
    branches: [ main, master, develop, 'claude/**' ]
  pull_request:
    branches: [ main, master, develop ]
  workflow_dispatch:

jobs:
  validate:
    name: Reference Validation
    runs-on: ubuntu-latest
    timeout-minutes: 30

    strategy:
      matrix:
        python-version: ['3.9', '3.10', '3.11']

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install numpy scipy matplotlib pytest
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      - name: Run reference validation
        id: validation
        run: |
          python test_voacap_reference.py --quiet
          echo "validation_completed=true" >> $GITHUB_OUTPUT

      - name: Check validation results
        if: steps.validation.outputs.validation_completed == 'true'
        run: |
          # Extract pass rate from validation results
          if [ -f validation_reference_results.json ]; then
            PASS_RATE=$(python -c "import json; print(json.load(open('validation_reference_results.json'))['summary']['pass_rate'])")
            echo "Pass rate: ${PASS_RATE}%"

            # Check against minimum target (80%)
            python -c "import sys; import json; \
              result = json.load(open('validation_reference_results.json')); \
              pass_rate = result['summary']['pass_rate']; \
              min_target = result['targets']['minimum_pass_rate']; \
              sys.exit(0 if pass_rate >= min_target else 1)"
          else
            echo "ERROR: validation_reference_results.json not found"
            exit 1
          fi

      - name: Upload validation results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: validation-results-py${{ matrix.python-version }}
          path: validation_reference_results.json
          retention-days: 30

      - name: Generate validation report
        if: always()
        run: |
          echo "## Validation Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f validation_reference_results.json ]; then
            python -c "
import json
import sys

with open('validation_reference_results.json') as f:
    results = json.load(f)

summary = results['summary']
targets = results['targets']

print(f\"**Test Cases Run:** {summary['test_cases_run']}\")
print(f\"**Total Comparisons:** {summary['total']}\")
print(f\"**Passed:** {summary['passed']} ({summary['pass_rate']:.1f}%)\")
print(f\"**Failed:** {summary['failed']}\")
print()

pass_rate = summary['pass_rate']
if pass_rate >= targets['excellent_pass_rate']:
    print(f\"✅ **EXCELLENT** - Pass rate {pass_rate:.1f}% exceeds target {targets['excellent_pass_rate']}%\")
elif pass_rate >= targets['target_pass_rate']:
    print(f\"✅ **VERY GOOD** - Pass rate {pass_rate:.1f}% exceeds target {targets['target_pass_rate']}%\")
elif pass_rate >= targets['minimum_pass_rate']:
    print(f\"✅ **PASSED** - Pass rate {pass_rate:.1f}% meets minimum {targets['minimum_pass_rate']}%\")
else:
    print(f\"❌ **BELOW TARGET** - Pass rate {pass_rate:.1f}% below minimum {targets['minimum_pass_rate']}%\")
" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ Validation failed to produce results" >> $GITHUB_STEP_SUMMARY
          fi

  test-suite-status:
    name: Test Suite Status
    runs-on: ubuntu-latest
    needs: validate
    if: always()

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Check test coverage
        run: |
          python -c "
import json

with open('test_config.json') as f:
    config = json.load(f)

total_tests = len(config['test_cases'])
active_tests = len([tc for tc in config['test_cases'] if tc.get('status') == 'active'])
pending_tests = total_tests - active_tests

print(f'Total test cases defined: {total_tests}')
print(f'Active test cases: {active_tests}')
print(f'Pending reference data: {pending_tests}')

if active_tests == 0:
    print('WARNING: No active test cases')
    exit(1)
"

      - name: Report test suite status
        run: |
          echo "## Test Suite Status" >> $GITHUB_STEP_SUMMARY
          python -c "
import json

with open('test_config.json') as f:
    config = json.load(f)

test_cases = config['test_cases']
active = [tc for tc in test_cases if tc.get('status') == 'active']
pending = [tc for tc in test_cases if tc.get('status') == 'pending_reference']

print(f\"**Total Test Cases:** {len(test_cases)}\")
print(f\"**Active:** {len(active)}\")
print(f\"**Pending Reference Data:** {len(pending)}\")
print()
print('### Active Test Cases:')
for tc in active:
    print(f\"- ✅ {tc['id']}: {tc['name']}\")
print()
print('### Pending Test Cases:')
for tc in pending:
    print(f\"- ⏳ {tc['id']}: {tc['name']}\")
" >> $GITHUB_STEP_SUMMARY
